tokenizer:
    zh: bert-base-chinese
    en: bert-base-uncased
pretrained_model:
    zh: bert-base-chinese
    en: bert-base-uncased
full_turn_length: 100

inference:
    seed: 0
    batch_size: 32
    res_max_len: 64
    min_len: 2
    min_mask_num: 1
    max_mask_num: 32
    min_masked_lm_prob: 0.3
    max_masked_lm_prob: 0.5
    augmentation_t: 10
    dropout: 0.1
    checkpoint:
        path: bert-fp/best_bert-base-chinese.pt
        is_load: true
